import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix,  accuracy_score

df = pd.read_csv("/content/Churn_Modelling.csv")
df = pd.read_csv('/content/Churn_Modelling.csv', header = 0)
print(df.head())

# Calculate or engineer the missing features (example)
df['Z_Score'] = (df['Balance'] - df['Balance'].mean()) / df['Balance'].std()
df['Balance_to_Salary_Ratio'] = df['Balance'] / df['EstimatedSalary']
df['Age_Tenure_Ratio'] = df['Age'] / df['Tenure']
df['CreditScore_Age'] = df['CreditScore'] / df['Age']

# Handle potential infinities
df.replace([np.inf, -np.inf], np.nan, inplace=True)  
# Replace infinities with NaN
df.dropna(subset=scaled_features, inplace=True) 
# Drop rows with NaN in scaled features

scaler = StandardScaler()
scaled_features = ['CreditScore', 'Age', 'Balance', 'NumOfProducts', 'EstimatedSalary', 'Z_Score', 'Balance_to_Salary_Ratio', 'Age_Tenure_Ratio', 'CreditScore_Age']
df[scaled_features] = scaler.fit_transform(df[scaled_features])
df.head()
log_reg = LogisticRegression(random_state=42)
rf = RandomForestClassifier(random_state=42)
# Identify columns with object (string) dtype
object_columns = df.select_dtypes(include=['object']).columns
print(object_columns)

# Handle non-numerical columns (example using one-hot encoding)
df = pd.get_dummies(df, columns=object_columns, drop_first=True)

# Recreate X_train and X_test after handling non-numerical columns
X = df.drop(columns='Exited')
y = df['Exited']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# Now try fitting the model again
log_reg.fit(X_train, y_train)
rf.fit(X_train, y_train)
y_pred_log_reg = log_reg.predict(X_test)
y_pred_random_forest = rf.predict(X_test)
# Logistic Regression
print("Logistic Regression - Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_log_reg))
print("\nLogistic Regression - Classification Report:")
print(classification_report(y_test, y_pred_log_reg))
# Random Forest
print("Random Forest - Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_random_forest))
print("\nRandom Forest - Classification Report:")
print(classification_report(y_test,y_pred_random_forest  ))
